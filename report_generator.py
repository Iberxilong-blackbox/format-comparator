import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from collections import Counter
import math

class ReportGenerator:
    """Generates reports and visualizations from format comparison differences."""

    def __init__(self, differences, metadata=None):
        """
        Initializes the ReportGenerator.

        Args:
            differences (list): A list of difference dictionaries from FormatComparator.
                                Each dict should have 'property', 'expected_value', 'actual_value',
                                'target_style_name', 'mapping_method', 'paragraph_index', 
                                'paragraph_text_preview', and optionally 'location_detail'.
                                The 'severity' key, if used by older versions, will be ignored here
                                as scoring logic is self-contained or uses raw error counts.
            metadata (dict, optional): Document metadata (e.g., total paragraphs, fonts).
                                       Defaults to None.
        """
        self.differences = differences if differences is not None else []
        
        # if self.differences:
        #     print(f"DEBUG (ReportGenerator Init): First difference: {self.differences[0]}")
        self.metadata = metadata if metadata is not None else {}
        self.df_diff = pd.DataFrame(self.differences) if self.differences else pd.DataFrame()
        
        # Ensure 'property' column exists even if df_diff is empty for consistency
        if 'property' not in self.df_diff.columns:
             self.df_diff['property'] = pd.Series(dtype='str')
        # Ensure 'severity' column exists for compatibility with scoring, default to 'Error' if not present
        # In the new win32com flow, 'severity' might not be explicitly set by FormatComparatorWin32
        # The scoring logic here will primarily rely on the presence of a difference.
        if 'severity' not in self.df_diff.columns:
            self.df_diff['severity'] = 'Error' # Assume all differences are errors for scoring

        # print(f"DEBUG (ReportGenerator Init): Total differences in DataFrame: {len(self.df_diff)}")
        # if not self.df_diff.empty:
        #     print(f"DEBUG (ReportGenerator Init): Columns in DataFrame: {list(self.df_diff.columns)}")
        #     print(f"DEBUG (ReportGenerator Init): First row: {self.df_diff.iloc[0].to_dict() if len(self.df_diff) > 0 else 'N/A'}")

    def get_summary_stats(self):
        """Calculates summary statistics for the report."""
        stats = {
            'total_paragraphs': self.metadata.get('total_paragraphs', 'N/A'), # From DocxReaderWin32
            'total_errors': 0, # All differences are treated as errors for this simplified version
            'total_warnings': 0, # Warnings are not explicitly generated by FormatComparatorWin32
            'blank_lines': self.metadata.get('blank_lines', 'N/A'), # Potentially from DocxReaderWin32
            'fonts_used': self.metadata.get('fonts_used', ['N/A']), # Potentially from DocxReaderWin32
            'error_density': 0.0,
            'error_counts_by_prop': {},
        }
        
        if not self.df_diff.empty:
            # All differences are considered errors for scoring in this simplified model
            stats['total_errors'] = len(self.df_diff) 
            
            total_items = stats['total_paragraphs']
            if isinstance(total_items, int) and total_items > 0:
                stats['error_density'] = stats['total_errors'] / total_items
            else:
                stats['error_density'] = 0.0

            if 'property' in self.df_diff.columns:
                stats['error_counts_by_prop'] = self.df_diff['property'].value_counts().to_dict()
        return stats

    def calculate_score_and_comment(self, error_weights, penalty_tiers, acceleration_thresholds):
        """
        Calculates a numerical score and generates a qualitative comment based on errors.
        Args are kept for compatibility, but logic might simplify.
        """
        base_score = 100.0
        total_penalty = 0.0
        
        summary_stats = self.get_summary_stats()
        error_counts_by_prop = summary_stats['error_counts_by_prop']
        error_density = summary_stats['error_density']
        total_errors = summary_stats['total_errors']
        
        for prop, count in error_counts_by_prop.items():
            base_weight = error_weights.get(prop, error_weights.get('默认', 1.0))
            penalty_multiplier = 1.0
            for threshold, multiplier in sorted(penalty_tiers):
                if count <= threshold:
                    penalty_multiplier = multiplier
                    break
            penalty = count * base_weight * penalty_multiplier
            total_penalty += penalty

        acceleration_multiplier = 1.0
        if math.isfinite(error_density):
            for density_threshold, multiplier in sorted(acceleration_thresholds):
                if error_density <= density_threshold:
                    acceleration_multiplier = multiplier
                    break
        
        final_penalty = total_penalty * acceleration_multiplier
        score = base_score - final_penalty

        comment_parts = []
        if score >= 95: comment_parts.append("优秀！文档格式非常规范。")
        elif score >= 80: comment_parts.append("良好。文档格式基本符合要求。")
        elif score >= 60: comment_parts.append("中等。文档格式存在一些问题。")
        elif score >= 40: comment_parts.append("较差。文档格式问题较多。")
        else: comment_parts.append("严重！文档格式存在大量错误。")

        if total_errors > 0:
            comment_parts.append(f"共发现 {total_errors} 处格式问题。")
            if acceleration_multiplier > 1.0 and math.isfinite(error_density):
                 comment_parts.append(f"问题密度较高 ({error_density:.1%})，总扣分已加重 (x{acceleration_multiplier:.1f})。")
        else:
             comment_parts.append("未发现格式问题。")
        
        comment = " ".join(comment_parts)
        return round(score, 1), comment

    def _get_color_for_score(self, score):
        if score >= 80: return '#10B981' 
        elif score >= 60: return '#F59E0B'
        elif score >= 40: return '#EF4444'
        else: return '#DC2626'

    def get_bar_chart_data(self):
        if self.df_diff.empty or 'property' not in self.df_diff.columns:
            return {'labels': [], 'values': []}
        
        # All diffs are considered for the chart in this version
        prop_counts = self.df_diff['property'].value_counts().sort_values(ascending=False)
        if prop_counts.empty:
            return {'labels': [], 'values': []}

        return {
            'labels': prop_counts.index.tolist(),
            'values': prop_counts.values.tolist()
        }

    def plot_errors_by_property_from_data(self, chart_data, score):
        if not chart_data or 'labels' not in chart_data or 'values' not in chart_data or not chart_data['labels']:
             return None

        labels = chart_data['labels']
        values = chart_data['values']
        title_color = self._get_color_for_score(score)

        fig = px.bar(
            x=labels, y=values, title="<b>主要问题类型分布 (按数量)</b>",
            labels={'x': '问题属性', 'y': '问题数量'}, text_auto=True,
            color_discrete_sequence=px.colors.qualitative.Pastel
        )
        fig.update_layout(
            title={'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top', 'font': {'size': 20, 'color': title_color}},
            xaxis_title="问题属性", yaxis_title="问题数量",
            xaxis={'categoryorder':'total descending'},
            plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)',
            font=dict(family="Arial, sans-serif", size=12, color="#374151"),
            yaxis=dict(gridcolor='#E5E7EB'), margin=dict(l=40, r=40, t=80, b=40)
        )
        fig.update_traces(textposition='outside')
        return fig

    def get_pie_chart_data(self):
        return self.get_bar_chart_data() # Same data source

    def plot_errors_by_property_pie_from_data(self, chart_data, score):
        if not chart_data or 'labels' not in chart_data or 'values' not in chart_data or not chart_data['labels']:
             return None

        labels = chart_data['labels']
        values = chart_data['values']
        title_color = self._get_color_for_score(score)

        fig = px.pie(
            names=labels, values=values, title="<b>主要问题类型分布 (按占比)</b>", hole=0.3
        )
        fig.update_layout(
             title={'y':0.95, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top', 'font': {'size': 20, 'color': title_color}},
             legend_title_text='问题属性',
             plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)',
             font=dict(family="Arial, sans-serif", size=12, color="#374151"),
             margin=dict(l=40, r=40, t=80, b=40)
        )
        fig.update_traces(textposition='inside', textinfo='percent+label', hoverinfo='label+percent+value')
        return fig

# Default scoring parameters (can be loaded from a config file in a real app)
DEFAULT_ERROR_WEIGHTS = {
    '字体.大小': 2.0, '字体.西文字体': 1.5, '字体.中文字体': 1.5,
    '段落.行间距': 1.0, '段落.首行缩进': 1.2, '段落.对齐方式': 0.8,
    '默认': 1.0 # Default weight for properties not listed
}
DEFAULT_PENALTY_TIERS = [
    (2, 0.5),      # 0-2 errors of a type: 0.5x penalty factor
    (10, 1.0),     # 3-10 errors: 1.0x penalty factor
    (float('inf'), 2.0) # >10 errors: 2.0x penalty factor
]
DEFAULT_ACCELERATION_THRESHOLDS = [
    (0.05, 1.0),   # Overall error density <= 5%: 1.0x total penalty
    (0.10, 1.2),   # Overall error density <= 10%: 1.2x total penalty
    (float('inf'), 1.5) # Overall error density > 10%: 1.5x total penalty
]

# --- Streamlit display functions (can be called from app.py) ---
def display_report_summary(st, score, comment, summary_stats):
    """Displays the summary card in Streamlit."""
    title_color = ReportGenerator(None)._get_color_for_score(score) # Temp instance for color
    
    st.markdown(f"""
    <div style="border: 2px solid {title_color}; border-radius: 10px; padding: 20px; margin-bottom: 20px; background-color: #f9fafb;">
        <h2 style="color: {title_color}; text-align: center;">格式评估总览</h2>
        <p style="font-size: 28px; font-weight: bold; text-align: center; color: {title_color}; margin-bottom: 10px;">
            得分: {score:.1f}
        </p>
        <p style="font-size: 16px; text-align: center; color: #4B5563; margin-bottom: 15px;">{comment}</p>
        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; font-size: 14px;">
            <div><strong>总段落数:</strong> {summary_stats.get('total_paragraphs', 'N/A')}</div>
            <div><strong>总问题数:</strong> {summary_stats.get('total_errors', 'N/A')}</div>
            <div><strong>问题密度:</strong> {summary_stats.get('error_density', 0.0):.2%}</div>
            <div><strong>空白行数:</strong> {summary_stats.get('blank_lines', 'N/A')}</div>
        </div>
    </div>
    """, unsafe_allow_html=True)

def display_report_charts(st, report_generator_instance, score):
    """Displays the charts in Streamlit."""
    st.subheader("📊 问题类型分布图")
    col1, col2 = st.columns(2)
    
    bar_data = report_generator_instance.get_bar_chart_data()
    fig_bar = report_generator_instance.plot_errors_by_property_from_data(bar_data, score)
    if fig_bar:
        with col1:
            st.plotly_chart(fig_bar, use_container_width=True)
    else:
        with col1:
            st.info("未发现问题，无法生成柱状图。")

    pie_data = report_generator_instance.get_pie_chart_data()
    fig_pie = report_generator_instance.plot_errors_by_property_pie_from_data(pie_data, score)
    if fig_pie:
        with col2:
            st.plotly_chart(fig_pie, use_container_width=True)
    else:
        with col2:
            st.info("未发现问题，无法生成饼图。")

def display_report_details_table(st, differences_df):
    """Displays the detailed differences table in Streamlit."""
    if not differences_df.empty:
        st.subheader("📋 详细问题列表")
        
        # Select and rename columns for display
        display_df = differences_df.copy()
        display_df["段落号"] = display_df["paragraph_index"] # Keep 1-based for display
        display_df["段落预览"] = display_df["paragraph_text_preview"].apply(lambda x: x[:50] + '...' if len(x) > 50 else x)
        display_df["问题属性"] = display_df["property"]
        display_df["预期值"] = display_df["expected_value"]
        display_df["实际值"] = display_df["actual_value"]
        display_df["目标样式"] = display_df["target_style_name"]
        # display_df["定位细节"] = display_df.get("location_detail", pd.Series(dtype='str')) # Handle missing column

        cols_to_display = ["段落号", "段落预览", "问题属性", "预期值", "实际值", "目标样式"]
        # if "定位细节" in display_df.columns:
        #     cols_to_display.append("定位细节")
        
        st.dataframe(display_df[cols_to_display], use_container_width=True, height=400)
    else:
        st.info("未发现详细格式问题。")

if __name__ == '__main__':
    # This example is for direct script testing and might need Streamlit context if st.* functions are called.
    # For simplicity, the Streamlit display functions are not called directly here.
    # Their logic is tested via report_app.py.
    
    sample_diffs = [
        {'property': '字体.大小', 'expected_value': '12pt', 'actual_value': '10pt', 'target_style_name': '正文', 'mapping_method': 'P1', 'paragraph_index': 5, 'paragraph_text_preview': '这是第五段的文本...'},
        {'property': '段落.行间距', 'expected_value': '1.5 倍', 'actual_value': '单倍', 'target_style_name': '正文', 'mapping_method': 'P1', 'paragraph_index': 8, 'paragraph_text_preview': '这是第八段的一些内容...'},
        {'property': '字体.西文字体', 'expected_value': 'Times New Roman', 'actual_value': 'Calibri', 'target_style_name': '正文', 'mapping_method': 'P1', 'paragraph_index': 5, 'paragraph_text_preview': '这是第五段的文本...', 'location_detail': "Run 0 (文本: '这是')"},
    ]
    sample_metadata = {'total_paragraphs': 50, 'blank_lines': 3, 'fonts_used': ['宋体', 'Calibri']}

    report_gen = ReportGenerator(sample_diffs, sample_metadata)
    stats = report_gen.get_summary_stats()
    score, comment = report_gen.calculate_score_and_comment(
        DEFAULT_ERROR_WEIGHTS, DEFAULT_PENALTY_TIERS, DEFAULT_ACCELERATION_THRESHOLDS
    )

    print("--- ReportGenerator Test ---")
    print(f"Score: {score}, Comment: {comment}")
    print("Summary Stats:", stats)
    
    bar_data = report_gen.get_bar_chart_data()
    print("Bar Chart Data:", bar_data)
    # fig = report_gen.plot_errors_by_property_from_data(bar_data, score)
    # if fig: fig.show()

    print("\n--- Test with no differences ---")
    report_gen_no_diff = ReportGenerator([], sample_metadata)
    stats_no_diff = report_gen_no_diff.get_summary_stats()
    score_no_diff, comment_no_diff = report_gen_no_diff.calculate_score_and_comment(
         DEFAULT_ERROR_WEIGHTS, DEFAULT_PENALTY_TIERS, DEFAULT_ACCELERATION_THRESHOLDS
    )
    print(f"Score (no diff): {score_no_diff}, Comment: {comment_no_diff}")
    print("Summary Stats (no diff):", stats_no_diff)
    bar_data_no_diff = report_gen_no_diff.get_bar_chart_data()
    print("Bar Chart Data (no diff):", bar_data_no_diff)