import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from collections import Counter
import math

class ReportGenerator:
    """Generates reports and visualizations from format comparison differences."""

    def __init__(self, differences, metadata=None):
        """
        Initializes the ReportGenerator.

        Args:
            differences (list): A list of difference dictionaries from FormatComparator.
                                Each dict should have 'property', 'expected_value', 'actual_value',
                                'target_style_name', 'mapping_method', 'paragraph_index', 
                                'paragraph_text_preview', and optionally 'location_detail'.
                                The 'severity' key, if used by older versions, will be ignored here
                                as scoring logic is self-contained or uses raw error counts.
            metadata (dict, optional): Document metadata (e.g., total paragraphs, fonts).
                                       Defaults to None.
        """
        self.differences = differences if differences is not None else []
        
        # if self.differences:
        #     print(f"DEBUG (ReportGenerator Init): First difference: {self.differences[0]}")
        self.metadata = metadata if metadata is not None else {}
        self.df_diff = pd.DataFrame(self.differences) if self.differences else pd.DataFrame()
        
        # Ensure 'property' column exists even if df_diff is empty for consistency
        if 'property' not in self.df_diff.columns:
             self.df_diff['property'] = pd.Series(dtype='str')
        # Ensure 'severity' column exists for compatibility with scoring, default to 'Error' if not present
        # In the new win32com flow, 'severity' might not be explicitly set by FormatComparatorWin32
        # The scoring logic here will primarily rely on the presence of a difference.
        if 'severity' not in self.df_diff.columns:
            self.df_diff['severity'] = 'Error' # Assume all differences are errors for scoring

        # print(f"DEBUG (ReportGenerator Init): Total differences in DataFrame: {len(self.df_diff)}")
        # if not self.df_diff.empty:
        #     print(f"DEBUG (ReportGenerator Init): Columns in DataFrame: {list(self.df_diff.columns)}")
        #     print(f"DEBUG (ReportGenerator Init): First row: {self.df_diff.iloc[0].to_dict() if len(self.df_diff) > 0 else 'N/A'}")

    def get_summary_stats(self):
        """Calculates summary statistics for the report."""
        stats = {
            'total_paragraphs': self.metadata.get('total_paragraphs', 'N/A'), # From DocxReaderWin32
            'total_errors': 0, # All differences are treated as errors for this simplified version
            'total_warnings': 0, # Warnings are not explicitly generated by FormatComparatorWin32
            'blank_lines': self.metadata.get('blank_lines', 'N/A'), # Potentially from DocxReaderWin32
            'fonts_used': self.metadata.get('fonts_used', ['N/A']), # Potentially from DocxReaderWin32
            'error_density': 0.0,
            'error_counts_by_prop': {},
        }
        
        if not self.df_diff.empty:
            # All differences are considered errors for scoring in this simplified model
            stats['total_errors'] = len(self.df_diff) 
            
            total_items = stats['total_paragraphs']
            if isinstance(total_items, int) and total_items > 0:
                stats['error_density'] = stats['total_errors'] / total_items
            else:
                stats['error_density'] = 0.0

            if 'property' in self.df_diff.columns:
                stats['error_counts_by_prop'] = self.df_diff['property'].value_counts().to_dict()
        return stats

    def calculate_score_and_comment(self, error_weights, penalty_tiers, acceleration_thresholds):
        """
        Calculates a numerical score and generates a qualitative comment based on errors.
        Args are kept for compatibility, but logic might simplify.
        """
        base_score = 100.0
        total_penalty = 0.0
        
        summary_stats = self.get_summary_stats()
        error_counts_by_prop = summary_stats['error_counts_by_prop']
        error_density = summary_stats['error_density']
        total_errors = summary_stats['total_errors']
        
        for prop, count in error_counts_by_prop.items():
            base_weight = error_weights.get(prop, error_weights.get('é»˜è®¤', 1.0))
            penalty_multiplier = 1.0
            for threshold, multiplier in sorted(penalty_tiers):
                if count <= threshold:
                    penalty_multiplier = multiplier
                    break
            penalty = count * base_weight * penalty_multiplier
            total_penalty += penalty

        acceleration_multiplier = 1.0
        if math.isfinite(error_density):
            for density_threshold, multiplier in sorted(acceleration_thresholds):
                if error_density <= density_threshold:
                    acceleration_multiplier = multiplier
                    break
        
        final_penalty = total_penalty * acceleration_multiplier
        score = base_score - final_penalty

        comment_parts = []
        if score >= 95: comment_parts.append("ä¼˜ç§€ï¼æ–‡æ¡£æ ¼å¼éå¸¸è§„èŒƒã€‚")
        elif score >= 80: comment_parts.append("è‰¯å¥½ã€‚æ–‡æ¡£æ ¼å¼åŸºæœ¬ç¬¦åˆè¦æ±‚ã€‚")
        elif score >= 60: comment_parts.append("ä¸­ç­‰ã€‚æ–‡æ¡£æ ¼å¼å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚")
        elif score >= 40: comment_parts.append("è¾ƒå·®ã€‚æ–‡æ¡£æ ¼å¼é—®é¢˜è¾ƒå¤šã€‚")
        else: comment_parts.append("ä¸¥é‡ï¼æ–‡æ¡£æ ¼å¼å­˜åœ¨å¤§é‡é”™è¯¯ã€‚")

        if total_errors > 0:
            comment_parts.append(f"å…±å‘ç° {total_errors} å¤„æ ¼å¼é—®é¢˜ã€‚")
            if acceleration_multiplier > 1.0 and math.isfinite(error_density):
                 comment_parts.append(f"é—®é¢˜å¯†åº¦è¾ƒé«˜ ({error_density:.1%})ï¼Œæ€»æ‰£åˆ†å·²åŠ é‡ (x{acceleration_multiplier:.1f})ã€‚")
        else:
             comment_parts.append("æœªå‘ç°æ ¼å¼é—®é¢˜ã€‚")
        
        comment = " ".join(comment_parts)
        return round(score, 1), comment

    def _get_color_for_score(self, score):
        if score >= 80: return '#10B981' 
        elif score >= 60: return '#F59E0B'
        elif score >= 40: return '#EF4444'
        else: return '#DC2626'

    def get_bar_chart_data(self):
        if self.df_diff.empty or 'property' not in self.df_diff.columns:
            return {'labels': [], 'values': []}
        
        # All diffs are considered for the chart in this version
        prop_counts = self.df_diff['property'].value_counts().sort_values(ascending=False)
        if prop_counts.empty:
            return {'labels': [], 'values': []}

        return {
            'labels': prop_counts.index.tolist(),
            'values': prop_counts.values.tolist()
        }

    def plot_errors_by_property_from_data(self, chart_data, score):
        if not chart_data or 'labels' not in chart_data or 'values' not in chart_data or not chart_data['labels']:
             return None

        labels = chart_data['labels']
        values = chart_data['values']
        title_color = self._get_color_for_score(score)

        fig = px.bar(
            x=labels, y=values, title="<b>ä¸»è¦é—®é¢˜ç±»å‹åˆ†å¸ƒ (æŒ‰æ•°é‡)</b>",
            labels={'x': 'é—®é¢˜å±æ€§', 'y': 'é—®é¢˜æ•°é‡'}, text_auto=True,
            color_discrete_sequence=px.colors.qualitative.Pastel
        )
        fig.update_layout(
            title={'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top', 'font': {'size': 20, 'color': title_color}},
            xaxis_title="é—®é¢˜å±æ€§", yaxis_title="é—®é¢˜æ•°é‡",
            xaxis={'categoryorder':'total descending'},
            plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)',
            font=dict(family="Arial, sans-serif", size=12, color="#374151"),
            yaxis=dict(gridcolor='#E5E7EB'), margin=dict(l=40, r=40, t=80, b=40)
        )
        fig.update_traces(textposition='outside')
        return fig

    def get_pie_chart_data(self):
        return self.get_bar_chart_data() # Same data source

    def plot_errors_by_property_pie_from_data(self, chart_data, score):
        if not chart_data or 'labels' not in chart_data or 'values' not in chart_data or not chart_data['labels']:
             return None

        labels = chart_data['labels']
        values = chart_data['values']
        title_color = self._get_color_for_score(score)

        fig = px.pie(
            names=labels, values=values, title="<b>ä¸»è¦é—®é¢˜ç±»å‹åˆ†å¸ƒ (æŒ‰å æ¯”)</b>", hole=0.3
        )
        fig.update_layout(
             title={'y':0.95, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top', 'font': {'size': 20, 'color': title_color}},
             legend_title_text='é—®é¢˜å±æ€§',
             plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)',
             font=dict(family="Arial, sans-serif", size=12, color="#374151"),
             margin=dict(l=40, r=40, t=80, b=40)
        )
        fig.update_traces(textposition='inside', textinfo='percent+label', hoverinfo='label+percent+value')
        return fig

# Default scoring parameters (can be loaded from a config file in a real app)
DEFAULT_ERROR_WEIGHTS = {
    'å­—ä½“.å¤§å°': 2.0, 'å­—ä½“.è¥¿æ–‡å­—ä½“': 1.5, 'å­—ä½“.ä¸­æ–‡å­—ä½“': 1.5,
    'æ®µè½.è¡Œé—´è·': 1.0, 'æ®µè½.é¦–è¡Œç¼©è¿›': 1.2, 'æ®µè½.å¯¹é½æ–¹å¼': 0.8,
    'é»˜è®¤': 1.0 # Default weight for properties not listed
}
DEFAULT_PENALTY_TIERS = [
    (2, 0.5),      # 0-2 errors of a type: 0.5x penalty factor
    (10, 1.0),     # 3-10 errors: 1.0x penalty factor
    (float('inf'), 2.0) # >10 errors: 2.0x penalty factor
]
DEFAULT_ACCELERATION_THRESHOLDS = [
    (0.05, 1.0),   # Overall error density <= 5%: 1.0x total penalty
    (0.10, 1.2),   # Overall error density <= 10%: 1.2x total penalty
    (float('inf'), 1.5) # Overall error density > 10%: 1.5x total penalty
]

# --- Streamlit display functions (can be called from app.py) ---
def display_report_summary(st, score, comment, summary_stats):
    """Displays the summary card in Streamlit."""
    title_color = ReportGenerator(None)._get_color_for_score(score) # Temp instance for color
    
    st.markdown(f"""
    <div style="border: 2px solid {title_color}; border-radius: 10px; padding: 20px; margin-bottom: 20px; background-color: #f9fafb;">
        <h2 style="color: {title_color}; text-align: center;">æ ¼å¼è¯„ä¼°æ€»è§ˆ</h2>
        <p style="font-size: 28px; font-weight: bold; text-align: center; color: {title_color}; margin-bottom: 10px;">
            å¾—åˆ†: {score:.1f}
        </p>
        <p style="font-size: 16px; text-align: center; color: #4B5563; margin-bottom: 15px;">{comment}</p>
        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px; font-size: 14px;">
            <div><strong>æ€»æ®µè½æ•°:</strong> {summary_stats.get('total_paragraphs', 'N/A')}</div>
            <div><strong>æ€»é—®é¢˜æ•°:</strong> {summary_stats.get('total_errors', 'N/A')}</div>
            <div><strong>é—®é¢˜å¯†åº¦:</strong> {summary_stats.get('error_density', 0.0):.2%}</div>
            <div><strong>ç©ºç™½è¡Œæ•°:</strong> {summary_stats.get('blank_lines', 'N/A')}</div>
        </div>
    </div>
    """, unsafe_allow_html=True)

def display_report_charts(st, report_generator_instance, score):
    """Displays the charts in Streamlit."""
    st.subheader("ğŸ“Š é—®é¢˜ç±»å‹åˆ†å¸ƒå›¾")
    col1, col2 = st.columns(2)
    
    bar_data = report_generator_instance.get_bar_chart_data()
    fig_bar = report_generator_instance.plot_errors_by_property_from_data(bar_data, score)
    if fig_bar:
        with col1:
            st.plotly_chart(fig_bar, use_container_width=True)
    else:
        with col1:
            st.info("æœªå‘ç°é—®é¢˜ï¼Œæ— æ³•ç”ŸæˆæŸ±çŠ¶å›¾ã€‚")

    pie_data = report_generator_instance.get_pie_chart_data()
    fig_pie = report_generator_instance.plot_errors_by_property_pie_from_data(pie_data, score)
    if fig_pie:
        with col2:
            st.plotly_chart(fig_pie, use_container_width=True)
    else:
        with col2:
            st.info("æœªå‘ç°é—®é¢˜ï¼Œæ— æ³•ç”Ÿæˆé¥¼å›¾ã€‚")

def display_report_details_table(st, differences_df):
    """Displays the detailed differences table in Streamlit."""
    if not differences_df.empty:
        st.subheader("ğŸ“‹ è¯¦ç»†é—®é¢˜åˆ—è¡¨")
        
        # Select and rename columns for display
        display_df = differences_df.copy()
        display_df["æ®µè½å·"] = display_df["paragraph_index"] # Keep 1-based for display
        display_df["æ®µè½é¢„è§ˆ"] = display_df["paragraph_text_preview"].apply(lambda x: x[:50] + '...' if len(x) > 50 else x)
        display_df["é—®é¢˜å±æ€§"] = display_df["property"]
        display_df["é¢„æœŸå€¼"] = display_df["expected_value"]
        display_df["å®é™…å€¼"] = display_df["actual_value"]
        display_df["ç›®æ ‡æ ·å¼"] = display_df["target_style_name"]
        # display_df["å®šä½ç»†èŠ‚"] = display_df.get("location_detail", pd.Series(dtype='str')) # Handle missing column

        cols_to_display = ["æ®µè½å·", "æ®µè½é¢„è§ˆ", "é—®é¢˜å±æ€§", "é¢„æœŸå€¼", "å®é™…å€¼", "ç›®æ ‡æ ·å¼"]
        # if "å®šä½ç»†èŠ‚" in display_df.columns:
        #     cols_to_display.append("å®šä½ç»†èŠ‚")
        
        st.dataframe(display_df[cols_to_display], use_container_width=True, height=400)
    else:
        st.info("æœªå‘ç°è¯¦ç»†æ ¼å¼é—®é¢˜ã€‚")

if __name__ == '__main__':
    # This example is for direct script testing and might need Streamlit context if st.* functions are called.
    # For simplicity, the Streamlit display functions are not called directly here.
    # Their logic is tested via report_app.py.
    
    sample_diffs = [
        {'property': 'å­—ä½“.å¤§å°', 'expected_value': '12pt', 'actual_value': '10pt', 'target_style_name': 'æ­£æ–‡', 'mapping_method': 'P1', 'paragraph_index': 5, 'paragraph_text_preview': 'è¿™æ˜¯ç¬¬äº”æ®µçš„æ–‡æœ¬...'},
        {'property': 'æ®µè½.è¡Œé—´è·', 'expected_value': '1.5 å€', 'actual_value': 'å•å€', 'target_style_name': 'æ­£æ–‡', 'mapping_method': 'P1', 'paragraph_index': 8, 'paragraph_text_preview': 'è¿™æ˜¯ç¬¬å…«æ®µçš„ä¸€äº›å†…å®¹...'},
        {'property': 'å­—ä½“.è¥¿æ–‡å­—ä½“', 'expected_value': 'Times New Roman', 'actual_value': 'Calibri', 'target_style_name': 'æ­£æ–‡', 'mapping_method': 'P1', 'paragraph_index': 5, 'paragraph_text_preview': 'è¿™æ˜¯ç¬¬äº”æ®µçš„æ–‡æœ¬...', 'location_detail': "Run 0 (æ–‡æœ¬: 'è¿™æ˜¯')"},
    ]
    sample_metadata = {'total_paragraphs': 50, 'blank_lines': 3, 'fonts_used': ['å®‹ä½“', 'Calibri']}

    report_gen = ReportGenerator(sample_diffs, sample_metadata)
    stats = report_gen.get_summary_stats()
    score, comment = report_gen.calculate_score_and_comment(
        DEFAULT_ERROR_WEIGHTS, DEFAULT_PENALTY_TIERS, DEFAULT_ACCELERATION_THRESHOLDS
    )

    print("--- ReportGenerator Test ---")
    print(f"Score: {score}, Comment: {comment}")
    print("Summary Stats:", stats)
    
    bar_data = report_gen.get_bar_chart_data()
    print("Bar Chart Data:", bar_data)
    # fig = report_gen.plot_errors_by_property_from_data(bar_data, score)
    # if fig: fig.show()

    print("\n--- Test with no differences ---")
    report_gen_no_diff = ReportGenerator([], sample_metadata)
    stats_no_diff = report_gen_no_diff.get_summary_stats()
    score_no_diff, comment_no_diff = report_gen_no_diff.calculate_score_and_comment(
         DEFAULT_ERROR_WEIGHTS, DEFAULT_PENALTY_TIERS, DEFAULT_ACCELERATION_THRESHOLDS
    )
    print(f"Score (no diff): {score_no_diff}, Comment: {comment_no_diff}")
    print("Summary Stats (no diff):", stats_no_diff)
    bar_data_no_diff = report_gen_no_diff.get_bar_chart_data()
    print("Bar Chart Data (no diff):", bar_data_no_diff)